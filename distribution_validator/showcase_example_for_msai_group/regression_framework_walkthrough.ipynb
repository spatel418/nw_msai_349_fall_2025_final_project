{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression Framework Walkthrough\n",
    "\n",
    "This notebook provides a comprehensive walkthrough of our regression framework. It's designed to help team members understand the code structure, functionality, and how to use it effectively for different regression tasks.\n",
    "\n",
    "## Table of Contents\n",
    "1. [Setup and Dependencies](#setup)\n",
    "2. [Overview of the Framework](#overview)\n",
    "3. [Data Preparation and Exploration](#data-prep)\n",
    "4. [Framework Components](#components)\n",
    "   - [RegressionBuilder](#builder)\n",
    "   - [RegressionTrain](#train)\n",
    "   - [RegressionInference](#inference)\n",
    "   - [RegressionHyperparameterTune](#tuning)\n",
    "5. [Key Features](#features)\n",
    "   - [Model Selection](#model-selection)\n",
    "   - [Feature Selection](#feature-selection)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Dependencies <a id=\"setup\"></a>\n",
    "\n",
    "Let's start by importing the necessary libraries and dependencies. The regression framework relies on various libraries for data manipulation, model training, and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import core libraries\n",
    "import os\n",
    "import shutil\n",
    "import json\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import warnings\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Configure warnings and logging\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Display settings for better visibility\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.width', 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import sklearn components\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\n",
    "from sklearn.feature_selection import SequentialFeatureSelector, RFE, RFECV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.linear_model import LinearRegression, BayesianRidge, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.exceptions import DataConversionWarning\n",
    "import xgboost as xgb\n",
    "# Note: TabPFN might require separate installation\n",
    "# Uncommenting this will ensure the notebook works even if TabPFN isn't available\n",
    "# try:\n",
    "#     from tabpfn import TabPFNRegressor\n",
    "#     from tabpfn_feature_importance_helper import TabPFNRegressorWithImportance\n",
    "#     TABPFN_AVAILABLE = True\n",
    "# except ImportError:\n",
    "#     print(\"TabPFN not available. Some functionality will be limited.\")\n",
    "#     TABPFN_AVAILABLE = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Overview of the Framework <a id=\"overview\"></a>\n",
    "\n",
    "Our regression framework is designed to streamline the process of building, training, and evaluating regression models. It provides a unified interface for various regression algorithms and incorporates best practices for model selection, hyperparameter tuning, and performance evaluation.\n",
    "\n",
    "### Key Components:\n",
    "\n",
    "1. **RegressionBuilder**: The main entry point that orchestrates the entire process\n",
    "2. **RegressionTrain**: Handles model training, validation, and selection\n",
    "3. **RegressionInference**: Manages prediction on new data\n",
    "4. **RegressionHyperparameterTune**: Optimizes model hyperparameters\n",
    "\n",
    "### Design Philosophy:\n",
    "- **Modularity**: Each component has a specific responsibility\n",
    "- **Flexibility**: Supports various regression algorithms and configurations\n",
    "- **Robustness**: Includes validation, error handling, and diagnostics\n",
    "- **Reproducibility**: Maintains consistent evaluation methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's first generate some sample data to use throughout this walkthrough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can use a real-world dataset like the California Housing dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "California Housing dataset shape: (20640, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8.3252</td>\n",
       "      <td>41.0</td>\n",
       "      <td>6.984127</td>\n",
       "      <td>1.023810</td>\n",
       "      <td>322.0</td>\n",
       "      <td>2.555556</td>\n",
       "      <td>37.88</td>\n",
       "      <td>-122.23</td>\n",
       "      <td>4.526</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8.3014</td>\n",
       "      <td>21.0</td>\n",
       "      <td>6.238137</td>\n",
       "      <td>0.971880</td>\n",
       "      <td>2401.0</td>\n",
       "      <td>2.109842</td>\n",
       "      <td>37.86</td>\n",
       "      <td>-122.22</td>\n",
       "      <td>3.585</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7.2574</td>\n",
       "      <td>52.0</td>\n",
       "      <td>8.288136</td>\n",
       "      <td>1.073446</td>\n",
       "      <td>496.0</td>\n",
       "      <td>2.802260</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.24</td>\n",
       "      <td>3.521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5.6431</td>\n",
       "      <td>52.0</td>\n",
       "      <td>5.817352</td>\n",
       "      <td>1.073059</td>\n",
       "      <td>558.0</td>\n",
       "      <td>2.547945</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3.8462</td>\n",
       "      <td>52.0</td>\n",
       "      <td>6.281853</td>\n",
       "      <td>1.081081</td>\n",
       "      <td>565.0</td>\n",
       "      <td>2.181467</td>\n",
       "      <td>37.85</td>\n",
       "      <td>-122.25</td>\n",
       "      <td>3.422</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   MedInc  HouseAge  AveRooms  AveBedrms  Population  AveOccup  Latitude  Longitude  target\n",
       "0  8.3252      41.0  6.984127   1.023810       322.0  2.555556     37.88    -122.23   4.526\n",
       "1  8.3014      21.0  6.238137   0.971880      2401.0  2.109842     37.86    -122.22   3.585\n",
       "2  7.2574      52.0  8.288136   1.073446       496.0  2.802260     37.85    -122.24   3.521\n",
       "3  5.6431      52.0  5.817352   1.073059       558.0  2.547945     37.85    -122.25   3.413\n",
       "4  3.8462      52.0  6.281853   1.081081       565.0  2.181467     37.85    -122.25   3.422"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load California Housing dataset\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "housing = fetch_california_housing()\n",
    "df_california = pd.DataFrame(housing.data, columns=housing.feature_names)\n",
    "df_california['target'] = housing.target\n",
    "target = \"target\"\n",
    "# Display the first few rows\n",
    "print(f\"California Housing dataset shape: {df_california.shape}\")\n",
    "df_california.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Preparation and Exploration <a id=\"data-prep\"></a>\n",
    "\n",
    "Before diving into the regression framework, let's explore the data to understand its characteristics. This step is crucial for making informed decisions about model selection and preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>MedInc</th>\n",
       "      <th>HouseAge</th>\n",
       "      <th>AveRooms</th>\n",
       "      <th>AveBedrms</th>\n",
       "      <th>Population</th>\n",
       "      <th>AveOccup</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "      <td>20640.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.870671</td>\n",
       "      <td>28.639486</td>\n",
       "      <td>5.429000</td>\n",
       "      <td>1.096675</td>\n",
       "      <td>1425.476744</td>\n",
       "      <td>3.070655</td>\n",
       "      <td>35.631861</td>\n",
       "      <td>-119.569704</td>\n",
       "      <td>2.068558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.899822</td>\n",
       "      <td>12.585558</td>\n",
       "      <td>2.474173</td>\n",
       "      <td>0.473911</td>\n",
       "      <td>1132.462122</td>\n",
       "      <td>10.386050</td>\n",
       "      <td>2.135952</td>\n",
       "      <td>2.003532</td>\n",
       "      <td>1.153956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.499900</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.846154</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.692308</td>\n",
       "      <td>32.540000</td>\n",
       "      <td>-124.350000</td>\n",
       "      <td>0.149990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.563400</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>4.440716</td>\n",
       "      <td>1.006079</td>\n",
       "      <td>787.000000</td>\n",
       "      <td>2.429741</td>\n",
       "      <td>33.930000</td>\n",
       "      <td>-121.800000</td>\n",
       "      <td>1.196000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.534800</td>\n",
       "      <td>29.000000</td>\n",
       "      <td>5.229129</td>\n",
       "      <td>1.048780</td>\n",
       "      <td>1166.000000</td>\n",
       "      <td>2.818116</td>\n",
       "      <td>34.260000</td>\n",
       "      <td>-118.490000</td>\n",
       "      <td>1.797000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.743250</td>\n",
       "      <td>37.000000</td>\n",
       "      <td>6.052381</td>\n",
       "      <td>1.099526</td>\n",
       "      <td>1725.000000</td>\n",
       "      <td>3.282261</td>\n",
       "      <td>37.710000</td>\n",
       "      <td>-118.010000</td>\n",
       "      <td>2.647250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>15.000100</td>\n",
       "      <td>52.000000</td>\n",
       "      <td>141.909091</td>\n",
       "      <td>34.066667</td>\n",
       "      <td>35682.000000</td>\n",
       "      <td>1243.333333</td>\n",
       "      <td>41.950000</td>\n",
       "      <td>-114.310000</td>\n",
       "      <td>5.000010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             MedInc      HouseAge      AveRooms     AveBedrms    Population      AveOccup      Latitude     Longitude        target\n",
       "count  20640.000000  20640.000000  20640.000000  20640.000000  20640.000000  20640.000000  20640.000000  20640.000000  20640.000000\n",
       "mean       3.870671     28.639486      5.429000      1.096675   1425.476744      3.070655     35.631861   -119.569704      2.068558\n",
       "std        1.899822     12.585558      2.474173      0.473911   1132.462122     10.386050      2.135952      2.003532      1.153956\n",
       "min        0.499900      1.000000      0.846154      0.333333      3.000000      0.692308     32.540000   -124.350000      0.149990\n",
       "25%        2.563400     18.000000      4.440716      1.006079    787.000000      2.429741     33.930000   -121.800000      1.196000\n",
       "50%        3.534800     29.000000      5.229129      1.048780   1166.000000      2.818116     34.260000   -118.490000      1.797000\n",
       "75%        4.743250     37.000000      6.052381      1.099526   1725.000000      3.282261     37.710000   -118.010000      2.647250\n",
       "max       15.000100     52.000000    141.909091     34.066667  35682.000000   1243.333333     41.950000   -114.310000      5.000010"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's use the California Housing dataset for our examples\n",
    "df = df_california.copy()\n",
    "\n",
    "# Basic statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Or use one of the chemistry datasets\n",
    "Likely you will have to modify the path based on your file system.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "chemistry_dataset_ex_1 = \"/mnt/c/Users/16303/misc/antonio_molecules/chemetrian/notebooks/nw_msai_349_fall_2025_final_project/vaskas.csv\"\n",
    "df = pd.read_csv(chemistry_dataset_ex_1)\n",
    "target = \"barrier\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chi-0_fa</th>\n",
       "      <th>chi-1_fa</th>\n",
       "      <th>chi-2_fa</th>\n",
       "      <th>chi-3_fa</th>\n",
       "      <th>chi-4_fa</th>\n",
       "      <th>chi-5_fa</th>\n",
       "      <th>Z-0_fa</th>\n",
       "      <th>Z-1_fa</th>\n",
       "      <th>Z-2_fa</th>\n",
       "      <th>Z-3_fa</th>\n",
       "      <th>Z-4_fa</th>\n",
       "      <th>Z-5_fa</th>\n",
       "      <th>I-0_fa</th>\n",
       "      <th>I-1_fa</th>\n",
       "      <th>I-2_fa</th>\n",
       "      <th>I-3_fa</th>\n",
       "      <th>I-4_fa</th>\n",
       "      <th>I-5_fa</th>\n",
       "      <th>T-0_fa</th>\n",
       "      <th>T-1_fa</th>\n",
       "      <th>T-2_fa</th>\n",
       "      <th>T-3_fa</th>\n",
       "      <th>T-4_fa</th>\n",
       "      <th>T-5_fa</th>\n",
       "      <th>S-0_fa</th>\n",
       "      <th>S-1_fa</th>\n",
       "      <th>S-2_fa</th>\n",
       "      <th>S-3_fa</th>\n",
       "      <th>S-4_fa</th>\n",
       "      <th>S-5_fa</th>\n",
       "      <th>chi-0_ma</th>\n",
       "      <th>chi-1_ma</th>\n",
       "      <th>chi-2_ma</th>\n",
       "      <th>chi-3_ma</th>\n",
       "      <th>chi-4_ma</th>\n",
       "      <th>chi-5_ma</th>\n",
       "      <th>Z-0_ma</th>\n",
       "      <th>Z-1_ma</th>\n",
       "      <th>Z-2_ma</th>\n",
       "      <th>Z-3_ma</th>\n",
       "      <th>Z-4_ma</th>\n",
       "      <th>Z-5_ma</th>\n",
       "      <th>I-0_ma</th>\n",
       "      <th>I-1_ma</th>\n",
       "      <th>I-2_ma</th>\n",
       "      <th>I-3_ma</th>\n",
       "      <th>I-4_ma</th>\n",
       "      <th>I-5_ma</th>\n",
       "      <th>T-0_ma</th>\n",
       "      <th>T-1_ma</th>\n",
       "      <th>T-2_ma</th>\n",
       "      <th>T-3_ma</th>\n",
       "      <th>T-4_ma</th>\n",
       "      <th>T-5_ma</th>\n",
       "      <th>S-0_ma</th>\n",
       "      <th>S-1_ma</th>\n",
       "      <th>S-2_ma</th>\n",
       "      <th>S-3_ma</th>\n",
       "      <th>S-4_ma</th>\n",
       "      <th>S-5_ma</th>\n",
       "      <th>chi-0_md</th>\n",
       "      <th>chi-1_md</th>\n",
       "      <th>chi-2_md</th>\n",
       "      <th>chi-3_md</th>\n",
       "      <th>chi-4_md</th>\n",
       "      <th>chi-5_md</th>\n",
       "      <th>Z-0_md</th>\n",
       "      <th>Z-1_md</th>\n",
       "      <th>Z-2_md</th>\n",
       "      <th>Z-3_md</th>\n",
       "      <th>Z-4_md</th>\n",
       "      <th>Z-5_md</th>\n",
       "      <th>I-0_md</th>\n",
       "      <th>I-1_md</th>\n",
       "      <th>I-2_md</th>\n",
       "      <th>I-3_md</th>\n",
       "      <th>I-4_md</th>\n",
       "      <th>I-5_md</th>\n",
       "      <th>T-0_md</th>\n",
       "      <th>T-1_md</th>\n",
       "      <th>T-2_md</th>\n",
       "      <th>T-3_md</th>\n",
       "      <th>T-4_md</th>\n",
       "      <th>T-5_md</th>\n",
       "      <th>S-0_md</th>\n",
       "      <th>S-1_md</th>\n",
       "      <th>S-2_md</th>\n",
       "      <th>S-3_md</th>\n",
       "      <th>S-4_md</th>\n",
       "      <th>S-5_md</th>\n",
       "      <th>distance</th>\n",
       "      <th>barrier</th>\n",
       "      <th>smiles</th>\n",
       "      <th>filename</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>271.4006</td>\n",
       "      <td>564.775</td>\n",
       "      <td>1031.2006</td>\n",
       "      <td>1395.6522</td>\n",
       "      <td>1972.4306</td>\n",
       "      <td>2482.2086</td>\n",
       "      <td>6923.0</td>\n",
       "      <td>7298.0</td>\n",
       "      <td>10116.0</td>\n",
       "      <td>12454.0</td>\n",
       "      <td>8122.0</td>\n",
       "      <td>4844.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>238.0</td>\n",
       "      <td>332.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>268.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>1042.0</td>\n",
       "      <td>1390.0</td>\n",
       "      <td>1574.0</td>\n",
       "      <td>1454.0</td>\n",
       "      <td>17.0209</td>\n",
       "      <td>44.4116</td>\n",
       "      <td>72.0370</td>\n",
       "      <td>98.2048</td>\n",
       "      <td>122.7906</td>\n",
       "      <td>123.6412</td>\n",
       "      <td>271.4006</td>\n",
       "      <td>564.775</td>\n",
       "      <td>1031.2006</td>\n",
       "      <td>1395.6522</td>\n",
       "      <td>1972.4306</td>\n",
       "      <td>2482.2086</td>\n",
       "      <td>6923.0</td>\n",
       "      <td>7298.0</td>\n",
       "      <td>10116.0</td>\n",
       "      <td>12454.0</td>\n",
       "      <td>8122.0</td>\n",
       "      <td>4844.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>238.0</td>\n",
       "      <td>332.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>268.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>1042.0</td>\n",
       "      <td>1390.0</td>\n",
       "      <td>1574.0</td>\n",
       "      <td>1454.0</td>\n",
       "      <td>17.0209</td>\n",
       "      <td>44.4116</td>\n",
       "      <td>72.0370</td>\n",
       "      <td>98.2048</td>\n",
       "      <td>122.7906</td>\n",
       "      <td>123.6412</td>\n",
       "      <td>271.4006</td>\n",
       "      <td>564.775</td>\n",
       "      <td>1031.2006</td>\n",
       "      <td>1395.6522</td>\n",
       "      <td>1972.4306</td>\n",
       "      <td>2482.2086</td>\n",
       "      <td>6923.0</td>\n",
       "      <td>7298.0</td>\n",
       "      <td>10116.0</td>\n",
       "      <td>12454.0</td>\n",
       "      <td>8122.0</td>\n",
       "      <td>4844.0</td>\n",
       "      <td>46.0</td>\n",
       "      <td>92.0</td>\n",
       "      <td>176.0</td>\n",
       "      <td>238.0</td>\n",
       "      <td>332.0</td>\n",
       "      <td>420.0</td>\n",
       "      <td>268.0</td>\n",
       "      <td>692.0</td>\n",
       "      <td>1042.0</td>\n",
       "      <td>1390.0</td>\n",
       "      <td>1574.0</td>\n",
       "      <td>1454.0</td>\n",
       "      <td>17.0209</td>\n",
       "      <td>44.4116</td>\n",
       "      <td>72.0370</td>\n",
       "      <td>98.2048</td>\n",
       "      <td>122.7906</td>\n",
       "      <td>123.6412</td>\n",
       "      <td>0.9348</td>\n",
       "      <td>14.8</td>\n",
       "      <td>[Ir]([P+](CC)(CC)CC)([C-]1N(C)CCN(C)1)(C#[O+])...</td>\n",
       "      <td>ir_tbp_1_dft-pet3_1_dft-sime_1_dft-co_1_dft-ic...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>224.3694</td>\n",
       "      <td>482.135</td>\n",
       "      <td>884.7308</td>\n",
       "      <td>1137.1092</td>\n",
       "      <td>1673.2128</td>\n",
       "      <td>1550.3932</td>\n",
       "      <td>6811.0</td>\n",
       "      <td>5588.0</td>\n",
       "      <td>8986.0</td>\n",
       "      <td>10374.0</td>\n",
       "      <td>5364.0</td>\n",
       "      <td>3814.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>218.0</td>\n",
       "      <td>556.0</td>\n",
       "      <td>858.0</td>\n",
       "      <td>1078.0</td>\n",
       "      <td>1094.0</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>14.5337</td>\n",
       "      <td>36.1862</td>\n",
       "      <td>57.4704</td>\n",
       "      <td>79.2868</td>\n",
       "      <td>92.0554</td>\n",
       "      <td>82.6380</td>\n",
       "      <td>224.3694</td>\n",
       "      <td>482.135</td>\n",
       "      <td>884.7308</td>\n",
       "      <td>1137.1092</td>\n",
       "      <td>1673.2128</td>\n",
       "      <td>1550.3932</td>\n",
       "      <td>6811.0</td>\n",
       "      <td>5588.0</td>\n",
       "      <td>8986.0</td>\n",
       "      <td>10374.0</td>\n",
       "      <td>5364.0</td>\n",
       "      <td>3814.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>218.0</td>\n",
       "      <td>556.0</td>\n",
       "      <td>858.0</td>\n",
       "      <td>1078.0</td>\n",
       "      <td>1094.0</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>14.5337</td>\n",
       "      <td>36.1862</td>\n",
       "      <td>57.4704</td>\n",
       "      <td>79.2868</td>\n",
       "      <td>92.0554</td>\n",
       "      <td>82.6380</td>\n",
       "      <td>224.3694</td>\n",
       "      <td>482.135</td>\n",
       "      <td>884.7308</td>\n",
       "      <td>1137.1092</td>\n",
       "      <td>1673.2128</td>\n",
       "      <td>1550.3932</td>\n",
       "      <td>6811.0</td>\n",
       "      <td>5588.0</td>\n",
       "      <td>8986.0</td>\n",
       "      <td>10374.0</td>\n",
       "      <td>5364.0</td>\n",
       "      <td>3814.0</td>\n",
       "      <td>38.0</td>\n",
       "      <td>76.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>186.0</td>\n",
       "      <td>286.0</td>\n",
       "      <td>252.0</td>\n",
       "      <td>218.0</td>\n",
       "      <td>556.0</td>\n",
       "      <td>858.0</td>\n",
       "      <td>1078.0</td>\n",
       "      <td>1094.0</td>\n",
       "      <td>1024.0</td>\n",
       "      <td>14.5337</td>\n",
       "      <td>36.1862</td>\n",
       "      <td>57.4704</td>\n",
       "      <td>79.2868</td>\n",
       "      <td>92.0554</td>\n",
       "      <td>82.6380</td>\n",
       "      <td>0.9699</td>\n",
       "      <td>12.4</td>\n",
       "      <td>[Ir]([N+](C)(C)C)([C-]1N(C)CCN(C)1)(C#[N+][H])...</td>\n",
       "      <td>ir_tbp_1_dft-nme3_1_dft-sime_1_dft-hicn_1_dft-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>305.2320</td>\n",
       "      <td>670.614</td>\n",
       "      <td>1125.6584</td>\n",
       "      <td>1522.5118</td>\n",
       "      <td>1856.8504</td>\n",
       "      <td>2402.9954</td>\n",
       "      <td>7215.0</td>\n",
       "      <td>8080.0</td>\n",
       "      <td>11196.0</td>\n",
       "      <td>13816.0</td>\n",
       "      <td>13400.0</td>\n",
       "      <td>11216.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>258.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>390.0</td>\n",
       "      <td>292.0</td>\n",
       "      <td>762.0</td>\n",
       "      <td>1172.0</td>\n",
       "      <td>1452.0</td>\n",
       "      <td>1702.0</td>\n",
       "      <td>1972.0</td>\n",
       "      <td>21.8694</td>\n",
       "      <td>56.4396</td>\n",
       "      <td>89.7068</td>\n",
       "      <td>117.5974</td>\n",
       "      <td>140.6520</td>\n",
       "      <td>164.6612</td>\n",
       "      <td>305.2320</td>\n",
       "      <td>670.614</td>\n",
       "      <td>1125.6584</td>\n",
       "      <td>1522.5118</td>\n",
       "      <td>1856.8504</td>\n",
       "      <td>2402.9954</td>\n",
       "      <td>7215.0</td>\n",
       "      <td>8080.0</td>\n",
       "      <td>11196.0</td>\n",
       "      <td>13816.0</td>\n",
       "      <td>13400.0</td>\n",
       "      <td>11216.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>258.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>390.0</td>\n",
       "      <td>292.0</td>\n",
       "      <td>762.0</td>\n",
       "      <td>1172.0</td>\n",
       "      <td>1452.0</td>\n",
       "      <td>1702.0</td>\n",
       "      <td>1972.0</td>\n",
       "      <td>21.8694</td>\n",
       "      <td>56.4396</td>\n",
       "      <td>89.7068</td>\n",
       "      <td>117.5974</td>\n",
       "      <td>140.6520</td>\n",
       "      <td>164.6612</td>\n",
       "      <td>305.2320</td>\n",
       "      <td>670.614</td>\n",
       "      <td>1125.6584</td>\n",
       "      <td>1522.5118</td>\n",
       "      <td>1856.8504</td>\n",
       "      <td>2402.9954</td>\n",
       "      <td>7215.0</td>\n",
       "      <td>8080.0</td>\n",
       "      <td>11196.0</td>\n",
       "      <td>13816.0</td>\n",
       "      <td>13400.0</td>\n",
       "      <td>11216.0</td>\n",
       "      <td>51.0</td>\n",
       "      <td>108.0</td>\n",
       "      <td>184.0</td>\n",
       "      <td>258.0</td>\n",
       "      <td>310.0</td>\n",
       "      <td>390.0</td>\n",
       "      <td>292.0</td>\n",
       "      <td>762.0</td>\n",
       "      <td>1172.0</td>\n",
       "      <td>1452.0</td>\n",
       "      <td>1702.0</td>\n",
       "      <td>1972.0</td>\n",
       "      <td>21.8694</td>\n",
       "      <td>56.4396</td>\n",
       "      <td>89.7068</td>\n",
       "      <td>117.5974</td>\n",
       "      <td>140.6520</td>\n",
       "      <td>164.6612</td>\n",
       "      <td>0.9983</td>\n",
       "      <td>5.2</td>\n",
       "      <td>[Ir]([P+](c1ccccc1)(c1ccccc1)c1ccccc1)([N+]1=C...</td>\n",
       "      <td>ir_tbp_1_dft-pph3_1_dft-oxaz_1_dft-hicn_1_dft-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>171.9665</td>\n",
       "      <td>354.089</td>\n",
       "      <td>617.2588</td>\n",
       "      <td>837.7394</td>\n",
       "      <td>1186.0020</td>\n",
       "      <td>751.2020</td>\n",
       "      <td>6893.0</td>\n",
       "      <td>8358.0</td>\n",
       "      <td>8314.0</td>\n",
       "      <td>6874.0</td>\n",
       "      <td>4074.0</td>\n",
       "      <td>1808.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>410.0</td>\n",
       "      <td>620.0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>610.0</td>\n",
       "      <td>472.0</td>\n",
       "      <td>12.4648</td>\n",
       "      <td>31.8018</td>\n",
       "      <td>49.8734</td>\n",
       "      <td>63.0708</td>\n",
       "      <td>63.2648</td>\n",
       "      <td>42.1078</td>\n",
       "      <td>171.9665</td>\n",
       "      <td>354.089</td>\n",
       "      <td>617.2588</td>\n",
       "      <td>837.7394</td>\n",
       "      <td>1186.0020</td>\n",
       "      <td>751.2020</td>\n",
       "      <td>6893.0</td>\n",
       "      <td>8358.0</td>\n",
       "      <td>8314.0</td>\n",
       "      <td>6874.0</td>\n",
       "      <td>4074.0</td>\n",
       "      <td>1808.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>410.0</td>\n",
       "      <td>620.0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>610.0</td>\n",
       "      <td>472.0</td>\n",
       "      <td>12.4648</td>\n",
       "      <td>31.8018</td>\n",
       "      <td>49.8734</td>\n",
       "      <td>63.0708</td>\n",
       "      <td>63.2648</td>\n",
       "      <td>42.1078</td>\n",
       "      <td>171.9665</td>\n",
       "      <td>354.089</td>\n",
       "      <td>617.2588</td>\n",
       "      <td>837.7394</td>\n",
       "      <td>1186.0020</td>\n",
       "      <td>751.2020</td>\n",
       "      <td>6893.0</td>\n",
       "      <td>8358.0</td>\n",
       "      <td>8314.0</td>\n",
       "      <td>6874.0</td>\n",
       "      <td>4074.0</td>\n",
       "      <td>1808.0</td>\n",
       "      <td>29.0</td>\n",
       "      <td>58.0</td>\n",
       "      <td>104.0</td>\n",
       "      <td>138.0</td>\n",
       "      <td>206.0</td>\n",
       "      <td>124.0</td>\n",
       "      <td>162.0</td>\n",
       "      <td>410.0</td>\n",
       "      <td>620.0</td>\n",
       "      <td>720.0</td>\n",
       "      <td>610.0</td>\n",
       "      <td>472.0</td>\n",
       "      <td>12.4648</td>\n",
       "      <td>31.8018</td>\n",
       "      <td>49.8734</td>\n",
       "      <td>63.0708</td>\n",
       "      <td>63.2648</td>\n",
       "      <td>42.1078</td>\n",
       "      <td>0.9353</td>\n",
       "      <td>10.5</td>\n",
       "      <td>[Ir]([n+]1ccncc1)([P+](C)(C)C)(C#[N+][H])([Cl]...</td>\n",
       "      <td>ir_tbp_1_dft-pyz_1_dft-pme3_1_dft-hicn_1_chlor...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>221.8709</td>\n",
       "      <td>472.145</td>\n",
       "      <td>822.1272</td>\n",
       "      <td>1033.6916</td>\n",
       "      <td>1544.7814</td>\n",
       "      <td>1473.7600</td>\n",
       "      <td>6755.0</td>\n",
       "      <td>6844.0</td>\n",
       "      <td>9234.0</td>\n",
       "      <td>9142.0</td>\n",
       "      <td>5510.0</td>\n",
       "      <td>3208.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>272.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>546.0</td>\n",
       "      <td>812.0</td>\n",
       "      <td>1026.0</td>\n",
       "      <td>1088.0</td>\n",
       "      <td>1092.0</td>\n",
       "      <td>14.8146</td>\n",
       "      <td>38.7222</td>\n",
       "      <td>60.6480</td>\n",
       "      <td>78.1554</td>\n",
       "      <td>91.8444</td>\n",
       "      <td>83.3710</td>\n",
       "      <td>221.8709</td>\n",
       "      <td>472.145</td>\n",
       "      <td>822.1272</td>\n",
       "      <td>1033.6916</td>\n",
       "      <td>1544.7814</td>\n",
       "      <td>1473.7600</td>\n",
       "      <td>6755.0</td>\n",
       "      <td>6844.0</td>\n",
       "      <td>9234.0</td>\n",
       "      <td>9142.0</td>\n",
       "      <td>5510.0</td>\n",
       "      <td>3208.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>272.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>546.0</td>\n",
       "      <td>812.0</td>\n",
       "      <td>1026.0</td>\n",
       "      <td>1088.0</td>\n",
       "      <td>1092.0</td>\n",
       "      <td>14.8146</td>\n",
       "      <td>38.7222</td>\n",
       "      <td>60.6480</td>\n",
       "      <td>78.1554</td>\n",
       "      <td>91.8444</td>\n",
       "      <td>83.3710</td>\n",
       "      <td>221.8709</td>\n",
       "      <td>472.145</td>\n",
       "      <td>822.1272</td>\n",
       "      <td>1033.6916</td>\n",
       "      <td>1544.7814</td>\n",
       "      <td>1473.7600</td>\n",
       "      <td>6755.0</td>\n",
       "      <td>6844.0</td>\n",
       "      <td>9234.0</td>\n",
       "      <td>9142.0</td>\n",
       "      <td>5510.0</td>\n",
       "      <td>3208.0</td>\n",
       "      <td>39.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>142.0</td>\n",
       "      <td>174.0</td>\n",
       "      <td>272.0</td>\n",
       "      <td>248.0</td>\n",
       "      <td>220.0</td>\n",
       "      <td>546.0</td>\n",
       "      <td>812.0</td>\n",
       "      <td>1026.0</td>\n",
       "      <td>1088.0</td>\n",
       "      <td>1092.0</td>\n",
       "      <td>14.8146</td>\n",
       "      <td>38.7222</td>\n",
       "      <td>60.6480</td>\n",
       "      <td>78.1554</td>\n",
       "      <td>91.8444</td>\n",
       "      <td>83.3710</td>\n",
       "      <td>0.9310</td>\n",
       "      <td>9.4</td>\n",
       "      <td>[Ir]([P+](C)(C)C)([C-]1N(C)C=CN(C)1)(C#[N+]C)(...</td>\n",
       "      <td>ir_tbp_1_dft-pme3_1_dft-ime_1_dft-iacn_1_dft-c...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   chi-0_fa  chi-1_fa   chi-2_fa   chi-3_fa   chi-4_fa   chi-5_fa  Z-0_fa  Z-1_fa   Z-2_fa   Z-3_fa   Z-4_fa   Z-5_fa  I-0_fa  I-1_fa  I-2_fa  I-3_fa  I-4_fa  I-5_fa  T-0_fa  T-1_fa  T-2_fa  T-3_fa  T-4_fa  T-5_fa   S-0_fa   S-1_fa   S-2_fa    S-3_fa    S-4_fa    S-5_fa  chi-0_ma  chi-1_ma   chi-2_ma   chi-3_ma   chi-4_ma   chi-5_ma  Z-0_ma  Z-1_ma   Z-2_ma   Z-3_ma   Z-4_ma   Z-5_ma  I-0_ma  I-1_ma  I-2_ma  I-3_ma  I-4_ma  I-5_ma  T-0_ma  T-1_ma  T-2_ma  T-3_ma  T-4_ma  T-5_ma   S-0_ma   S-1_ma   S-2_ma    S-3_ma    S-4_ma    S-5_ma  chi-0_md  chi-1_md   chi-2_md   chi-3_md   chi-4_md   chi-5_md  Z-0_md  Z-1_md   Z-2_md   Z-3_md   Z-4_md   Z-5_md  I-0_md  I-1_md  I-2_md  I-3_md  I-4_md  I-5_md  T-0_md  T-1_md  T-2_md  T-3_md  T-4_md  T-5_md   S-0_md   S-1_md   S-2_md    S-3_md    S-4_md    S-5_md  distance  barrier                                             smiles                                           filename\n",
       "0  271.4006   564.775  1031.2006  1395.6522  1972.4306  2482.2086  6923.0  7298.0  10116.0  12454.0   8122.0   4844.0    46.0    92.0   176.0   238.0   332.0   420.0   268.0   692.0  1042.0  1390.0  1574.0  1454.0  17.0209  44.4116  72.0370   98.2048  122.7906  123.6412  271.4006   564.775  1031.2006  1395.6522  1972.4306  2482.2086  6923.0  7298.0  10116.0  12454.0   8122.0   4844.0    46.0    92.0   176.0   238.0   332.0   420.0   268.0   692.0  1042.0  1390.0  1574.0  1454.0  17.0209  44.4116  72.0370   98.2048  122.7906  123.6412  271.4006   564.775  1031.2006  1395.6522  1972.4306  2482.2086  6923.0  7298.0  10116.0  12454.0   8122.0   4844.0    46.0    92.0   176.0   238.0   332.0   420.0   268.0   692.0  1042.0  1390.0  1574.0  1454.0  17.0209  44.4116  72.0370   98.2048  122.7906  123.6412    0.9348     14.8  [Ir]([P+](CC)(CC)CC)([C-]1N(C)CCN(C)1)(C#[O+])...  ir_tbp_1_dft-pet3_1_dft-sime_1_dft-co_1_dft-ic...\n",
       "1  224.3694   482.135   884.7308  1137.1092  1673.2128  1550.3932  6811.0  5588.0   8986.0  10374.0   5364.0   3814.0    38.0    76.0   142.0   186.0   286.0   252.0   218.0   556.0   858.0  1078.0  1094.0  1024.0  14.5337  36.1862  57.4704   79.2868   92.0554   82.6380  224.3694   482.135   884.7308  1137.1092  1673.2128  1550.3932  6811.0  5588.0   8986.0  10374.0   5364.0   3814.0    38.0    76.0   142.0   186.0   286.0   252.0   218.0   556.0   858.0  1078.0  1094.0  1024.0  14.5337  36.1862  57.4704   79.2868   92.0554   82.6380  224.3694   482.135   884.7308  1137.1092  1673.2128  1550.3932  6811.0  5588.0   8986.0  10374.0   5364.0   3814.0    38.0    76.0   142.0   186.0   286.0   252.0   218.0   556.0   858.0  1078.0  1094.0  1024.0  14.5337  36.1862  57.4704   79.2868   92.0554   82.6380    0.9699     12.4  [Ir]([N+](C)(C)C)([C-]1N(C)CCN(C)1)(C#[N+][H])...  ir_tbp_1_dft-nme3_1_dft-sime_1_dft-hicn_1_dft-...\n",
       "2  305.2320   670.614  1125.6584  1522.5118  1856.8504  2402.9954  7215.0  8080.0  11196.0  13816.0  13400.0  11216.0    51.0   108.0   184.0   258.0   310.0   390.0   292.0   762.0  1172.0  1452.0  1702.0  1972.0  21.8694  56.4396  89.7068  117.5974  140.6520  164.6612  305.2320   670.614  1125.6584  1522.5118  1856.8504  2402.9954  7215.0  8080.0  11196.0  13816.0  13400.0  11216.0    51.0   108.0   184.0   258.0   310.0   390.0   292.0   762.0  1172.0  1452.0  1702.0  1972.0  21.8694  56.4396  89.7068  117.5974  140.6520  164.6612  305.2320   670.614  1125.6584  1522.5118  1856.8504  2402.9954  7215.0  8080.0  11196.0  13816.0  13400.0  11216.0    51.0   108.0   184.0   258.0   310.0   390.0   292.0   762.0  1172.0  1452.0  1702.0  1972.0  21.8694  56.4396  89.7068  117.5974  140.6520  164.6612    0.9983      5.2  [Ir]([P+](c1ccccc1)(c1ccccc1)c1ccccc1)([N+]1=C...  ir_tbp_1_dft-pph3_1_dft-oxaz_1_dft-hicn_1_dft-...\n",
       "3  171.9665   354.089   617.2588   837.7394  1186.0020   751.2020  6893.0  8358.0   8314.0   6874.0   4074.0   1808.0    29.0    58.0   104.0   138.0   206.0   124.0   162.0   410.0   620.0   720.0   610.0   472.0  12.4648  31.8018  49.8734   63.0708   63.2648   42.1078  171.9665   354.089   617.2588   837.7394  1186.0020   751.2020  6893.0  8358.0   8314.0   6874.0   4074.0   1808.0    29.0    58.0   104.0   138.0   206.0   124.0   162.0   410.0   620.0   720.0   610.0   472.0  12.4648  31.8018  49.8734   63.0708   63.2648   42.1078  171.9665   354.089   617.2588   837.7394  1186.0020   751.2020  6893.0  8358.0   8314.0   6874.0   4074.0   1808.0    29.0    58.0   104.0   138.0   206.0   124.0   162.0   410.0   620.0   720.0   610.0   472.0  12.4648  31.8018  49.8734   63.0708   63.2648   42.1078    0.9353     10.5  [Ir]([n+]1ccncc1)([P+](C)(C)C)(C#[N+][H])([Cl]...  ir_tbp_1_dft-pyz_1_dft-pme3_1_dft-hicn_1_chlor...\n",
       "4  221.8709   472.145   822.1272  1033.6916  1544.7814  1473.7600  6755.0  6844.0   9234.0   9142.0   5510.0   3208.0    39.0    78.0   142.0   174.0   272.0   248.0   220.0   546.0   812.0  1026.0  1088.0  1092.0  14.8146  38.7222  60.6480   78.1554   91.8444   83.3710  221.8709   472.145   822.1272  1033.6916  1544.7814  1473.7600  6755.0  6844.0   9234.0   9142.0   5510.0   3208.0    39.0    78.0   142.0   174.0   272.0   248.0   220.0   546.0   812.0  1026.0  1088.0  1092.0  14.8146  38.7222  60.6480   78.1554   91.8444   83.3710  221.8709   472.145   822.1272  1033.6916  1544.7814  1473.7600  6755.0  6844.0   9234.0   9142.0   5510.0   3208.0    39.0    78.0   142.0   174.0   272.0   248.0   220.0   546.0   812.0  1026.0  1088.0  1092.0  14.8146  38.7222  60.6480   78.1554   91.8444   83.3710    0.9310      9.4  [Ir]([P+](C)(C)C)([C-]1N(C)C=CN(C)1)(C#[N+]C)(...  ir_tbp_1_dft-pme3_1_dft-ime_1_dft-iacn_1_dft-c..."
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Framework Components <a id=\"components\"></a>\n",
    "\n",
    "Let's explore each of the main components of our regression framework. We'll start by importing the necessary classes from our framework files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 RegressionBuilder <a id=\"builder\"></a>\n",
    "\n",
    "The `RegressionBuilder` class serves as the main entry point to our regression framework. It follows the builder pattern, allowing for clean method chaining and provides a unified interface to configure and run the regression process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a simplified version of the constants used by the framework\n",
    "# In practice, these would be imported from constants.py\n",
    "\n",
    "# Scoring metrics\n",
    "SCORING = {\n",
    "    'mae': 'neg_mean_absolute_error',\n",
    "    'mse': 'neg_mean_squared_error',\n",
    "    'rmse': 'neg_root_mean_squared_error',\n",
    "    'r2': 'r2',\n",
    "    'adj_r2': 'r2'  # Note: sklearn doesn't have adjusted R2 scorer; we handle this separately\n",
    "}\n",
    "\n",
    "# Regression metrics for evaluation\n",
    "OFFLINE_REGRESSION_METRICS = ['mae', 'mse', 'rmse', 'r2', 'adj_r2']\n",
    "\n",
    "# Default hyperparameters for each model type\n",
    "MODEL_PARAM_DEFAULT = {\n",
    "    'linear': {},  # Linear regression uses default parameters\n",
    "    'bayesian': {\n",
    "        'alpha_1': 1e-6, \n",
    "        'alpha_2': 1e-6,\n",
    "        'lambda_1': 1e-6, \n",
    "        'lambda_2': 1e-6\n",
    "    },\n",
    "    'elasticnet': {'alpha': 0.1, 'l1_ratio': 0.5},\n",
    "    'xgboost': {\n",
    "        'n_estimators': 100,\n",
    "        'learning_rate': 0.1,\n",
    "        'max_depth': 3,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8\n",
    "    },\n",
    "    'randomforest': {\n",
    "        'n_estimators': 100,\n",
    "        'max_depth': 10,\n",
    "        'min_samples_split': 2,\n",
    "        'min_samples_leaf': 1\n",
    "    },\n",
    "    'svm': {'C': 1.0, 'epsilon': 0.1, 'gamma': 'scale'},\n",
    "    'tabpfn': {'N_ensemble_configurations': 32}\n",
    "}\n",
    "\n",
    "# Hyperparameter ranges for tuning\n",
    "MODEL_PARAM_RANGE = {\n",
    "    'linear': {},  # No tuning for linear regression\n",
    "    'bayesian': {\n",
    "        'alpha_1': [1e-7, 1e-6, 1e-5, 1e-4, 1e-3],\n",
    "        'alpha_2': [1e-7, 1e-6, 1e-5, 1e-4, 1e-3],\n",
    "        'lambda_1': [1e-7, 1e-6, 1e-5, 1e-4, 1e-3],\n",
    "        'lambda_2': [1e-7, 1e-6, 1e-5, 1e-4, 1e-3]\n",
    "    },\n",
    "    'elasticnet': {\n",
    "        'alpha': [0.001, 0.01, 0.1, 0.5, 1.0],\n",
    "        'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9]\n",
    "    },\n",
    "    'xgboost': {\n",
    "        'n_estimators': [50, 100, 200, 300],\n",
    "        'learning_rate': [0.01, 0.05, 0.1, 0.2],\n",
    "        'max_depth': [3, 5, 7, 9],\n",
    "        'subsample': [0.6, 0.8, 1.0],\n",
    "        'colsample_bytree': [0.6, 0.8, 1.0]\n",
    "    },\n",
    "    'randomforest': {\n",
    "        'n_estimators': [50, 100, 200, 300],\n",
    "        'max_depth': [5, 10, 15, 20, None],\n",
    "        'min_samples_split': [2, 5, 10],\n",
    "        'min_samples_leaf': [1, 2, 4]\n",
    "    },\n",
    "    'svm': {\n",
    "        'C': [0.1, 1.0, 10.0, 100.0],\n",
    "        'epsilon': [0.01, 0.1, 0.2],\n",
    "        'gamma': ['scale', 'auto', 0.1, 0.01]\n",
    "    },\n",
    "    'tabpfn': {\n",
    "        'N_ensemble_configurations': [16, 32, 64, 128]\n",
    "    }\n",
    "}\n",
    "\n",
    "# Hyperparameters to not display in output\n",
    "MODEL_HYPERPARAMTERS_TO_NOT_DISPLAY = {\n",
    "    'linear': ['copy_X', 'n_jobs', 'positive'],\n",
    "    'bayesian': ['compute_score', 'fit_intercept', 'verbose'],\n",
    "    'elasticnet': ['copy_X', 'fit_intercept', 'precompute', 'selection', 'tol', 'warm_start'],\n",
    "    'xgboost': ['booster', 'verbosity', 'objective', 'nthread', 'gamma', 'min_child_weight'],\n",
    "    'randomforest': ['bootstrap', 'ccp_alpha', 'max_features', 'max_leaf_nodes', 'oob_score', 'verbose'],\n",
    "    'svm': ['kernel', 'degree', 'coef0', 'shrinking', 'tol', 'cache_size', 'verbose', 'max_iter'],\n",
    "    'tabpfn': ['device', 'base_path', 'verbose']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the purpose of this notebook, let's define the adjusted R² function\n",
    "def adj_r2(r2, n, p):\n",
    "    \"\"\"\n",
    "    Calculate adjusted R-squared\n",
    "    Parameters:\n",
    "    r2 (float): R-squared value\n",
    "    n (int): Sample size\n",
    "    p (int): Number of predictors (excluding intercept)\n",
    "    Returns:\n",
    "    float: Adjusted R-squared value\n",
    "    \"\"\"\n",
    "    # Check if we have enough samples relative to predictors\n",
    "    if n <= p + 1:\n",
    "        return float(\"nan\")  # Not enough degrees of freedom\n",
    "    # Correct formula for adjusted R-squared\n",
    "    adj_r2_value = 1 - ((1 - r2) * (n - 1) / (n - p - 1))\n",
    "    # Adjusted R² should never exceed R²\n",
    "    if adj_r2_value > r2:\n",
    "        return r2\n",
    "    return adj_r2_value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's examine the `RegressionBuilder` class to understand its structure and functionality:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simplified implementation of RegressionBuilder\n",
    "class RegressionBuilder:\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_training_df,       # Training data DataFrame\n",
    "        input_inference_df,      # Inference data DataFrame (can be None for training-only)\n",
    "        target,                  # Target column name\n",
    "        enable_parameter_tune,   # Whether to tune hyperparameters\n",
    "        data_augmentation,       # Whether to apply data augmentation\n",
    "        feature_selection_autoselect, # Whether to use automatic feature selection\n",
    "        feature_selection_num,   # Number of features to select (0 = auto)\n",
    "        goal_metric,             # Optimization metric ('mae', 'mse', 'r2', 'adj_r2')\n",
    "        data_augmentation_method = None,  # Method for augmentation ('gaussian', 'smote', 'vae')\n",
    "        data_augmentation_sectioning = None,  # Sectioning method ('binning', 'kde')\n",
    "        data_augmentation_region_num = 10,    # Number of regions for augmentation\n",
    "        data_augmentation_min_samples_per_region = 5,  # Min samples per region\n",
    "        balance_strategy = \"equal\",  # Strategy for balancing regions\n",
    "        k = 10,                 # Number of folds for cross-validation\n",
    "        sampling_method = 'kfold', # Sampling method ('kfold', 'stratified')\n",
    "        model_choice = None,    # Model to use (string or list)\n",
    "        saved_model = None,     # Pre-trained model for inference\n",
    "        saved_scaler = None,    # Pre-fitted scaler for inference\n",
    "        target_provided = False # Whether target is provided for inference data\n",
    "    ):\n",
    "        # Store all configuration parameters\n",
    "        self.input_training = input_training_df\n",
    "        self.input_inference = input_inference_df\n",
    "        self.target = target\n",
    "        self.enable_parameter_tune = enable_parameter_tune\n",
    "        self.data_augmentation = data_augmentation\n",
    "        self.feature_selection_autoselect = feature_selection_autoselect\n",
    "        self.feature_selection_num = feature_selection_num\n",
    "        self.goal_metric = goal_metric\n",
    "        self.data_augmentation_method = data_augmentation_method\n",
    "        self.data_augmentation_sectioning = data_augmentation_sectioning\n",
    "        self.data_augmentation_region_num = data_augmentation_region_num\n",
    "        self.data_augmentation_min_samples_per_region = data_augmentation_min_samples_per_region\n",
    "        self.balance_strategy = balance_strategy\n",
    "        self.k = k\n",
    "        self.sampling_method = sampling_method\n",
    "        self.model_choice = model_choice\n",
    "        self.saved_model = saved_model\n",
    "        self.saved_scaler = saved_scaler\n",
    "        self.target_provided = target_provided\n",
    "        \n",
    "        # File validation would be handled here in the full implementation\n",
    "        # self.regression_training_file_validator = ... \n",
    "        # self.regression_inference_file_validator = ...\n",
    "\n",
    "    def populate_model_dict(self):\n",
    "        \"\"\"Create a dictionary of model classes based on user selection\"\"\"\n",
    "        # Define available models\n",
    "        model_dict = {\n",
    "            \"linear\": LinearRegression,\n",
    "            \"bayesian\": BayesianRidge,\n",
    "            \"elasticnet\": ElasticNet,\n",
    "            \"xgboost\": xgb.XGBRegressor,\n",
    "            \"randomforest\": RandomForestRegressor,\n",
    "            \"svm\": SVR,\n",
    "            # \"tabpfn\": TabPFNRegressor  # Commented out to avoid dependency issues\n",
    "        }\n",
    "        \n",
    "        # Filter models based on user choice\n",
    "        if isinstance(self.model_choice, list):\n",
    "            return {k: {\"model\": model_dict[k]} for k in self.model_choice if k in model_dict}\n",
    "        elif isinstance(self.model_choice, str) and self.model_choice in model_dict:\n",
    "            return {self.model_choice: {\"model\": model_dict[self.model_choice]}}\n",
    "        else:\n",
    "            # Default: use all available models\n",
    "            return {k: {\"model\": v} for k, v in model_dict.items()}\n",
    "\n",
    "    def prepare_model_dict_for_training(self, model_dict):\n",
    "        \"\"\"Prepare the model dictionary with feature information\"\"\"\n",
    "        X = (\n",
    "            self.input_training.drop(columns=[self.target])\n",
    "            .select_dtypes(include=[\"number\"])\n",
    "            .fillna(0)\n",
    "        )\n",
    "        feature_dict = {k: X.columns for k in model_dict.keys()}\n",
    "        return {\n",
    "            model_name: {\n",
    "                \"model\": model_dict[model_name],\n",
    "                \"feature_analysis\": list(feature_dict.get(model_name, [])),\n",
    "            }\n",
    "            for model_name in model_dict\n",
    "        }\n",
    "\n",
    "    def build_model(self):\n",
    "        \"\"\"Build and train the regression model\"\"\"\n",
    "        model_dict = self.populate_model_dict()\n",
    "        \n",
    "        # In a full implementation, this would create a RegressionTrain instance\n",
    "        # and call find_best_model. For simplicity, we'll just outline the steps.\n",
    "        print(\"1. Initializing RegressionTrain with the following configuration:\")\n",
    "        print(f\"   - Target: {self.target}\")\n",
    "        print(f\"   - Hyperparameter tuning: {self.enable_parameter_tune}\")\n",
    "        print(f\"   - Data augmentation: {self.data_augmentation}\")\n",
    "        print(f\"   - Feature selection: {self.feature_selection_autoselect}\")\n",
    "        print(f\"   - Goal metric: {self.goal_metric}\")\n",
    "        print(f\"   - How are we splitting train and test?: {self.sampling_method}\")\n",
    "        print(f\"   - Models to evaluate: {list(model_dict.keys())}\")\n",
    "        \n",
    "        print(\"\\n2. Training and evaluating models:\")\n",
    "        # This would call regression_train.find_best_model(model_dict)\n",
    "        for model_name in model_dict.keys():\n",
    "            print(f\"   - Training {model_name}...\")\n",
    "        \n",
    "        print(f\"\\n3. Selecting best model based on {self.goal_metric}\")\n",
    "        print(\"4. Preparing final model and metrics\")\n",
    "        \n",
    "        # Placeholder for storing results\n",
    "        self.model_name = list(model_dict.keys())[0]  # Just for demonstration\n",
    "        self.model = None\n",
    "        self.scaler = MinMaxScaler()\n",
    "        self.metrics = {}\n",
    "        self.features = []\n",
    "        self.goal_metric_per_folds = {\"train\": [], \"test\": []}\n",
    "        self.true_prediction_point = {}\n",
    "        \n",
    "        return \"Model training process outlined\"\n",
    "\n",
    "    def run_inference(self):\n",
    "        \"\"\"Run inference on new data using a pre-trained model\"\"\"\n",
    "        print(\"Running inference with pre-trained model\")\n",
    "        # This would create a RegressionInference instance and call inference()\n",
    "        return {\"predictions\": []}  # Placeholder for demonstration\n",
    "\n",
    "    def run_regression(self):\n",
    "        \"\"\"Main method to execute either training or inference\"\"\"\n",
    "        if self.input_training is not None:\n",
    "            # Training mode\n",
    "            print(\"Running in training mode\")\n",
    "            self.build_model()\n",
    "            return {\n",
    "                \"metrics\": self.metrics,\n",
    "                \"model_name\": self.model_name,\n",
    "                \"model\": self.model,\n",
    "                \"hyperparameters\": [],  # Would be populated with model parameters\n",
    "                \"scaler\": self.scaler,\n",
    "                \"features\": self.features,\n",
    "                \"goal_metric_per_folds\": self.goal_metric_per_folds,\n",
    "                \"true_prediction_point\": self.true_prediction_point,\n",
    "            }\n",
    "        else:\n",
    "            # Inference mode\n",
    "            print(\"Running in inference mode\")\n",
    "            return self.run_inference()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's demonstrate how to use the `RegressionBuilder` class for a basic training run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Selected models: ['randomforest']\n",
      "1. Initializing RegressionTrain with the following configuration:\n",
      "   - Target: barrier\n",
      "   - Hyperparameter tuning: False\n",
      "   - Data augmentation: False\n",
      "   - Feature selection: False\n",
      "   - Goal metric: r2\n",
      "   - How are we splitting train and test?: random\n",
      "   - Models to evaluate: ['randomforest']\n",
      "\n",
      "2. Training and evaluating models:\n",
      "   - Training randomforest...\n",
      "\n",
      "3. Selecting best model based on r2\n",
      "4. Preparing final model and metrics\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Model training process outlined'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Split the data into train and test sets for demonstration\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Create a RegressionBuilder instance with basic configuration\n",
    "regression_builder = RegressionBuilder(\n",
    "    input_training_df=train_df,\n",
    "    input_inference_df=None,\n",
    "    target=target,\n",
    "    enable_parameter_tune=False,\n",
    "    data_augmentation=False,\n",
    "    feature_selection_autoselect=False,\n",
    "    feature_selection_num=0,\n",
    "    goal_metric='r2',\n",
    "    model_choice=['randomforest'],\n",
    "    sampling_method=\"random\"\n",
    ")\n",
    "\n",
    "# Let's see what models are selected\n",
    "model_dict = regression_builder.populate_model_dict()\n",
    "print(f\"Selected models: {list(model_dict.keys())}\")\n",
    "\n",
    "# Outline the training process\n",
    "regression_builder.build_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 RegressionTrain <a id=\"train\"></a>\n",
    "\n",
    "The `RegressionTrain` class is responsible for training regression models. It handles various aspects of the training process, including data preprocessing, feature selection, hyperparameter tuning, and cross-validation (if requested)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionTrain:\n",
    "    \"\"\"Class for training regression models with various options\"\"\"\n",
    "    \n",
    "    def __init__(\n",
    "        self,\n",
    "        input_df,                 # Input DataFrame\n",
    "        target,                   # Target column name\n",
    "        enable_parameter_tune,    # Whether to tune hyperparameters\n",
    "        data_augmentation,        # Whether to apply data augmentation\n",
    "        feature_selection_autoselect, # Whether to use automatic feature selection\n",
    "        feature_selection_num,    # Number of features to select (0 = auto)\n",
    "        goal_metric,              # Optimization metric\n",
    "        data_augmentation_method=None,  # Method for augmentation\n",
    "        data_augmentation_sectioning=None,  # Sectioning method\n",
    "        data_augmentation_region_num=10,    # Number of regions\n",
    "        data_augmentation_min_samples_per_region=5,  # Min samples per region\n",
    "        balance_strategy=\"equal\", # Strategy for balancing regions\n",
    "        k=10,                    # Number of folds for CV\n",
    "        sampling_method='kfold'  # Sampling method ('kfold', 'stratified')\n",
    "    ):\n",
    "        # Store configuration parameters\n",
    "        self.input = input_df\n",
    "        self.target = target\n",
    "        self.enable_parameter_tune = enable_parameter_tune\n",
    "        self.data_augmentation = data_augmentation\n",
    "        self.feature_selection_autoselect = feature_selection_autoselect\n",
    "        self.feature_selection_num = feature_selection_num\n",
    "        self.goal_metric = goal_metric\n",
    "        self.data_augmentation_method = data_augmentation_method\n",
    "        self.data_augmentation_sectioning = data_augmentation_sectioning\n",
    "        self.data_augmentation_region_num = data_augmentation_region_num\n",
    "        self.data_augmentation_min_samples_per_region = data_augmentation_min_samples_per_region\n",
    "        self.balance_strategy = balance_strategy\n",
    "        self.k = k\n",
    "        self.sampling_method = sampling_method\n",
    "\n",
    "    def normalize(self, X, scaler=None):\n",
    "        \"\"\"Normalize features using MinMaxScaler\"\"\"\n",
    "        if scaler is None:\n",
    "            scaler = MinMaxScaler()\n",
    "            X_scaled = scaler.fit_transform(X)\n",
    "            return X_scaled, scaler\n",
    "        else:\n",
    "            X_scaled = scaler.transform(X)\n",
    "            return X_scaled\n",
    "    \n",
    "    def find_best_model(self, model_dict):\n",
    "        \"\"\"Train and evaluate all models to find the best one\"\"\"\n",
    "        # Extract features and target\n",
    "        X = (\n",
    "            self.input.drop(columns=[self.target])\n",
    "            .select_dtypes(include=[\"number\"])\n",
    "            .fillna(0)\n",
    "        )\n",
    "        y = self.input[[self.target]].fillna(self.input[self.target].mean())\n",
    "        \n",
    "        # Initialize best model tracking\n",
    "        best_model = {\n",
    "            \"model_name\": None,\n",
    "            f\"test_{self.goal_metric}\": None,\n",
    "            \"goal_metric_per_folds\": None,\n",
    "            \"model\": None,\n",
    "            \"metrics\": None,\n",
    "            \"selected_features\": None,\n",
    "            \"test_true_predicted\": None,\n",
    "        }\n",
    "        \n",
    "        # Determine number of CV splits based on data size\n",
    "        num_splits = self.k if len(X) >= self.k else len(X)\n",
    "        \n",
    "        # Train and evaluate each model\n",
    "        for k in model_dict.keys():\n",
    "            print(f\"Training and evaluating {k} model...\")\n",
    "            # This would call self.train_model() in the full implementation\n",
    "            # For simplicity, we'll just outline the process\n",
    "            \n",
    "            # 1. Create a dummy model\n",
    "            if k == 'linear':\n",
    "                model = LinearRegression()\n",
    "            elif k == 'randomforest':\n",
    "                model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "            else:\n",
    "                # Default to a simple model\n",
    "                model = LinearRegression()\n",
    "                \n",
    "            # 2. Placeholder for metrics (would be calculated by train_model)\n",
    "            model_dict[k][\"metrics\"] = {\n",
    "                f\"test_{self.goal_metric}\": 0.8,  # Placeholder value\n",
    "                \"model\": model,\n",
    "                \"selected_features\": list(X.columns),\n",
    "                \"test_true_predicted\": {}\n",
    "            }\n",
    "            goal_metric_per_folds = {\"train\": [0.85], \"test\": [0.8]}  # Placeholder\n",
    "            \n",
    "            # 3. Check if this model is better than the current best\n",
    "            current_metric = model_dict[k][\"metrics\"][f\"test_{self.goal_metric}\"]\n",
    "            is_first_model = best_model[f\"test_{self.goal_metric}\"] is None\n",
    "            \n",
    "            # Determine if current model is better based on goal metric\n",
    "            if self.goal_metric in [\"mae\", \"mse\", \"rmse\"]:\n",
    "                # For error metrics, lower is better\n",
    "                is_better = (\n",
    "                    is_first_model\n",
    "                    or current_metric < best_model[f\"test_{self.goal_metric}\"]\n",
    "                )\n",
    "            else:  # 'r2' or 'adj_r2'\n",
    "                # For fit metrics, higher is better\n",
    "                is_better = (\n",
    "                    is_first_model\n",
    "                    or current_metric > best_model[f\"test_{self.goal_metric}\"]\n",
    "                )\n",
    "                \n",
    "            # 4. Update best model if needed\n",
    "            if is_better:\n",
    "                best_model[f\"test_{self.goal_metric}\"] = current_metric\n",
    "                best_model[\"model_name\"] = k\n",
    "                best_model[\"model\"] = model_dict[k][\"metrics\"][\"model\"]\n",
    "                best_model[\"metrics\"] = model_dict[k][\"metrics\"]\n",
    "                best_model[\"selected_features\"] = model_dict[k][\"metrics\"][\"selected_features\"]\n",
    "                best_model[\"goal_metric_per_folds\"] = goal_metric_per_folds\n",
    "                best_model[\"test_true_predicted\"] = model_dict[k][\"metrics\"][\"test_true_predicted\"]\n",
    "        \n",
    "        # Create a scaler based on the best features\n",
    "        scaler_result = self.normalize(X[best_model[\"selected_features\"]])\n",
    "        best_model[\"scaler\"] = scaler_result[1]\n",
    "        \n",
    "        return best_model\n",
    "    \n",
    "    def train_model(self, model_name, model_dict, X, y, n_splits):\n",
    "        \"\"\"Train a model using the appropriate sampling method\"\"\"\n",
    "        if hasattr(self, 'sampling_method') and self.sampling_method == \"stratified\":\n",
    "            return self._train_model_stratified(model_name, model_dict, X, y)\n",
    "        else:\n",
    "            return self._train_model_kfold(model_name, model_dict, X, y, n_splits)\n",
    "    \n",
    "    # For brevity, we're not including all the training methods here\n",
    "    # The full implementation would include _train_model_kfold, _train_model_stratified, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how the `RegressionTrain` class works with a simple example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training and evaluating randomforest model...\n",
      "\n",
      "Best model: randomforest\n"
     ]
    }
   ],
   "source": [
    "# Create a simple model dictionary for demonstration\n",
    "model_dict = {\n",
    "    'randomforest': {'model': RandomForestRegressor}\n",
    "}\n",
    "\n",
    "# Initialize RegressionTrain\n",
    "regression_train = RegressionTrain(\n",
    "    input_df=train_df,\n",
    "    target=target,\n",
    "    enable_parameter_tune=False,\n",
    "    data_augmentation=False,\n",
    "    feature_selection_autoselect=False,\n",
    "    feature_selection_num=0,\n",
    "    goal_metric='r2',\n",
    "    sampling_method=\"random\"\n",
    ")\n",
    "\n",
    "# Find the best model\n",
    "best_model = regression_train.find_best_model(model_dict)\n",
    "print(f\"\\nBest model: {best_model['model_name']}\")\n",
    "#print(f\"Selected features: {best_model['selected_features']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 RegressionHyperparameterTune <a id=\"tuning\"></a>\n",
    "\n",
    "The `RegressionHyperparameterTune` class is responsible for optimizing model hyperparameters using RandomizedSearchCV (Similar to GridSearch in class but performs much faster than GridSearch with a small loss in metric optimization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionHyperparameterTune:\n",
    "    \"\"\"Class for hyperparameter tuning using RandomizedSearchCV\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name, model, X, y, goal_metric, k=10):\n",
    "        self.model_name = model_name\n",
    "        self.model = model\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "        self.goal_metric = goal_metric\n",
    "        self.k = k\n",
    "    \n",
    "    def tune_parameters_rscv(self):\n",
    "        \"\"\"Tune hyperparameters using RandomizedSearchCV\"\"\"\n",
    "        print(f\"Tuning hyperparameters for {self.model_name} model...\")\n",
    "        \n",
    "        # Get parameter distributions from constants\n",
    "        param_distributions = MODEL_PARAM_RANGE.get(self.model_name, {})\n",
    "        \n",
    "        if not param_distributions:\n",
    "            print(f\"No tuning parameters defined for {self.model_name}. Using defaults.\")\n",
    "            return MODEL_PARAM_DEFAULT.get(self.model_name, {})\n",
    "        \n",
    "        # Set up RandomizedSearchCV\n",
    "        random_search = RandomizedSearchCV(\n",
    "            self.model(),\n",
    "            scoring=SCORING[self.goal_metric],\n",
    "            param_distributions=param_distributions,\n",
    "            n_iter=3,  # Reduced for demonstration\n",
    "            random_state=13,\n",
    "            cv=self.k if len(self.X) >= self.k else len(self.X),\n",
    "            verbose=1\n",
    "        )\n",
    "        \n",
    "        # Fit to data\n",
    "        random_search.fit(self.X, self.y)\n",
    "        \n",
    "        print(f\"Best parameters: {random_search.best_params_}\")\n",
    "        return random_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how to use the hyperparameter tuning class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tuning hyperparameters for randomforest model...\n",
      "Fitting 2 folds for each of 3 candidates, totalling 6 fits\n",
      "Best parameters: {'n_estimators': 200, 'min_samples_split': 5, 'min_samples_leaf': 4, 'max_depth': None}\n"
     ]
    }
   ],
   "source": [
    "# Prepare data for tuning\n",
    "X = train_df.drop(columns=[target]).select_dtypes(include=[\"number\"]).fillna(0)\n",
    "y = train_df[target]\n",
    "\n",
    "# Initialize tuner for RandomForest\n",
    "hyperparameter_tune = RegressionHyperparameterTune(\n",
    "    model_name='randomforest',\n",
    "    model=RandomForestRegressor,\n",
    "    X=X,\n",
    "    y=y,\n",
    "    goal_metric='r2',\n",
    "    k=2  # Using fewer folds for demonstration\n",
    ")\n",
    "\n",
    "# Tune hyperparameters\n",
    "best_params = hyperparameter_tune.tune_parameters_rscv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 RegressionInference <a id=\"inference\"></a>\n",
    "\n",
    "The `RegressionInference` class is responsible for making predictions on new data using a pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RegressionInference:\n",
    "    \"\"\"Class for making predictions with a pre-trained model\"\"\"\n",
    "    \n",
    "    def __init__(self, input_df, scaler, model, target, target_provided=False):\n",
    "        self.input = input_df\n",
    "        self.scaler = scaler\n",
    "        self.model = model\n",
    "        self.target = target\n",
    "        self.target_provided = target_provided\n",
    "    \n",
    "    def normalize(self, X):\n",
    "        \"\"\"Normalize features using the pre-fitted scaler\"\"\"\n",
    "        X_scaled = self.scaler.transform(X)\n",
    "        return X_scaled\n",
    "    \n",
    "    def inference(self):\n",
    "        \"\"\"Make predictions on the input data\"\"\"\n",
    "        # Prepare features\n",
    "        X = self.input.select_dtypes(include=[\"number\"]).fillna(0)\n",
    "        X = X[[_ for _ in X.columns if _ != self.target]]\n",
    "        X = pd.DataFrame(self.normalize(X), columns=X.columns)\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = [float(_) for _ in list(self.model.predict(X))]\n",
    "        \n",
    "        # If target is provided, calculate metrics\n",
    "        if self.target_provided:\n",
    "            y_test = self.input[[self.target]]\n",
    "            test_metrics = {\n",
    "                \"test_mae\": mean_absolute_error(y_test, predictions),\n",
    "                \"test_mse\": mean_squared_error(y_test, predictions),\n",
    "                \"test_adj_r2\": adj_r2(\n",
    "                    r2_score(y_test, predictions), len(y_test), len(X.columns)\n",
    "                ),\n",
    "                \"test_r2\": r2_score(y_test, predictions),\n",
    "                \"test_true_predicted\": [\n",
    "                    {\n",
    "                        \"true\": y_test[self.target].tolist()[i],\n",
    "                        \"predicted\": predictions[i],\n",
    "                    }\n",
    "                    for i in range(len(X))\n",
    "                ],\n",
    "            }\n",
    "            return {\"predictions\": predictions, \"test_metrics\": test_metrics}\n",
    "        else:\n",
    "            return {\"predictions\": predictions}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see how to use the inference class with a pre-trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of predictions: 390\n",
      "First few predictions: [7.943000000000004, 7.943000000000004, 7.943000000000004, 7.943000000000004, 7.943000000000004]\n",
      "\n",
      "Metrics:\n",
      "  test_mae: 4.927838461538458\n",
      "  test_mse: 38.36605079230766\n",
      "  test_adj_r2: -1.4590178303354504\n",
      "  test_r2: -0.8837720139844838\n"
     ]
    }
   ],
   "source": [
    "# Train a simple model to use for inference\n",
    "X_train = train_df.drop(columns=[target]).select_dtypes(include=[\"number\"]).fillna(0)\n",
    "y_train = train_df[target]\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Create a scaler\n",
    "scaler = MinMaxScaler()\n",
    "scaler.fit(X_train)\n",
    "\n",
    "# Initialize the inference class\n",
    "regression_inference = RegressionInference(\n",
    "    input_df=test_df,\n",
    "    scaler=scaler,\n",
    "    model=model,\n",
    "    target=target,\n",
    "    target_provided=True  # We have the true values for evaluation\n",
    ")\n",
    "\n",
    "# Make predictions\n",
    "results = regression_inference.inference()\n",
    "\n",
    "# Display results\n",
    "print(f\"Number of predictions: {len(results['predictions'])}\")\n",
    "print(f\"First few predictions: {results['predictions'][:5]}\")\n",
    "print(f\"\\nMetrics:\")\n",
    "for metric, value in results['test_metrics'].items():\n",
    "    if metric != 'test_true_predicted':\n",
    "        print(f\"  {metric}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Key Features <a id=\"features\"></a>\n",
    "\n",
    "Now that we've explored the main components of our regression framework, let's dive deeper into some of its key features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Model Selection <a id=\"model-selection\"></a>\n",
    "\n",
    "Our framework supports multiple regression algorithms, each with its own strengths and weaknesses. Here's a quick overview of the available models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Key in Framework</th>\n",
       "      <th>Best For</th>\n",
       "      <th>Strengths</th>\n",
       "      <th>Limitations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>linear</td>\n",
       "      <td>Simple linear relationships, interpretability</td>\n",
       "      <td>Fast, interpretable, low variance</td>\n",
       "      <td>Cannot capture nonlinear relationships</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bayesian Ridge</td>\n",
       "      <td>bayesian</td>\n",
       "      <td>Linear relationships with uncertainty estimates</td>\n",
       "      <td>Provides uncertainty estimates, robust to ill-...</td>\n",
       "      <td>Still assumes linear relationship</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Elastic Net</td>\n",
       "      <td>elasticnet</td>\n",
       "      <td>Linear relationships with sparse feature selec...</td>\n",
       "      <td>Feature selection via L1 regularization</td>\n",
       "      <td>Sensitive to hyperparameters</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>randomforest</td>\n",
       "      <td>Complex nonlinear relationships, robust to out...</td>\n",
       "      <td>Captures interactions, robust to outliers and ...</td>\n",
       "      <td>Can overfit, slow on large datasets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>XGBoost</td>\n",
       "      <td>xgboost</td>\n",
       "      <td>High performance on structured data, handles n...</td>\n",
       "      <td>High performance, feature importance, handles ...</td>\n",
       "      <td>Sensitive to hyperparameters, can overfit</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Support Vector Regression</td>\n",
       "      <td>svm</td>\n",
       "      <td>Nonlinear relationships, high-dimensional spaces</td>\n",
       "      <td>Works well in high dimensions, captures comple...</td>\n",
       "      <td>Slow for large datasets, sensitive to scaling</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>TabPFN</td>\n",
       "      <td>tabpfn</td>\n",
       "      <td>Small datasets, no hyperparameter tuning needed</td>\n",
       "      <td>Few-shot learning, no hyperparameter tuning, w...</td>\n",
       "      <td>Limited to smaller datasets, less interpretable</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model Name Key in Framework                                           Best For                                          Strengths                                      Limitations\n",
       "0          Linear Regression           linear      Simple linear relationships, interpretability                  Fast, interpretable, low variance           Cannot capture nonlinear relationships\n",
       "1             Bayesian Ridge         bayesian    Linear relationships with uncertainty estimates  Provides uncertainty estimates, robust to ill-...                Still assumes linear relationship\n",
       "2                Elastic Net       elasticnet  Linear relationships with sparse feature selec...            Feature selection via L1 regularization                     Sensitive to hyperparameters\n",
       "3              Random Forest     randomforest  Complex nonlinear relationships, robust to out...  Captures interactions, robust to outliers and ...              Can overfit, slow on large datasets\n",
       "4                    XGBoost          xgboost  High performance on structured data, handles n...  High performance, feature importance, handles ...        Sensitive to hyperparameters, can overfit\n",
       "5  Support Vector Regression              svm   Nonlinear relationships, high-dimensional spaces  Works well in high dimensions, captures comple...    Slow for large datasets, sensitive to scaling\n",
       "6                     TabPFN           tabpfn    Small datasets, no hyperparameter tuning needed  Few-shot learning, no hyperparameter tuning, w...  Limited to smaller datasets, less interpretable"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a DataFrame to describe the models\n",
    "model_descriptions = pd.DataFrame({\n",
    "    'Model Name': [\n",
    "        'Linear Regression', \n",
    "        'Bayesian Ridge', \n",
    "        'Elastic Net',\n",
    "        'Random Forest',\n",
    "        'XGBoost',\n",
    "        'Support Vector Regression',\n",
    "        'TabPFN'\n",
    "    ],\n",
    "    'Key in Framework': [\n",
    "        'linear', \n",
    "        'bayesian', \n",
    "        'elasticnet',\n",
    "        'randomforest',\n",
    "        'xgboost',\n",
    "        'svm',\n",
    "        'tabpfn'\n",
    "    ],\n",
    "    'Best For': [\n",
    "        'Simple linear relationships, interpretability', \n",
    "        'Linear relationships with uncertainty estimates', \n",
    "        'Linear relationships with sparse feature selection',\n",
    "        'Complex nonlinear relationships, robust to outliers',\n",
    "        'High performance on structured data, handles nonlinearity well',\n",
    "        'Nonlinear relationships, high-dimensional spaces',\n",
    "        'Small datasets, no hyperparameter tuning needed'\n",
    "    ],\n",
    "    'Strengths': [\n",
    "        'Fast, interpretable, low variance', \n",
    "        'Provides uncertainty estimates, robust to ill-posed problems', \n",
    "        'Feature selection via L1 regularization',\n",
    "        'Captures interactions, robust to outliers and noise',\n",
    "        'High performance, feature importance, handles missing values',\n",
    "        'Works well in high dimensions, captures complex patterns',\n",
    "        'Few-shot learning, no hyperparameter tuning, works well on small datasets'\n",
    "    ],\n",
    "    'Limitations': [\n",
    "        'Cannot capture nonlinear relationships', \n",
    "        'Still assumes linear relationship', \n",
    "        'Sensitive to hyperparameters',\n",
    "        'Can overfit, slow on large datasets',\n",
    "        'Sensitive to hyperparameters, can overfit',\n",
    "        'Slow for large datasets, sensitive to scaling',\n",
    "        'Limited to smaller datasets, less interpretable'\n",
    "    ]\n",
    "})\n",
    "\n",
    "model_descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The framework automatically evaluates multiple models and selects the best one based on the specified goal metric. This approach ensures that you don't have to manually try different algorithms to find the most suitable one for your specific dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Feature Selection <a id=\"feature-selection\"></a>\n",
    "\n",
    "Feature selection is a critical step in building effective regression models. Our framework supports both automatic and manual feature selection through the Recursive Feature Elimination (RFE) method.\n",
    "\n",
    "Feature Selection is built into the variants of train_model() where the program does not always run this process but if they set the class variable \"feature_selection_autoselect\" to true it will then run something similar to the below cell.\n",
    "\n",
    "#### I believe this method will be lightly touched upon in class as this is an automated feature selection method. The problem here is it can take a really long time to run given a large number of features, rows of data, and possibly the model. Random Forest I have seen typically performs much better with datasets found in chemistry but at the cost of time. If all number of features + rows of data are the same, auto feature selection like RFE or RFECV takes much longer for random forest than it does for linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['target'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[41]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfeature_selection\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m RFE\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Prepare data\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m X = \u001b[43mtrain_df\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtarget\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m.select_dtypes(include=[\u001b[33m\"\u001b[39m\u001b[33mnumber\u001b[39m\u001b[33m\"\u001b[39m]).fillna(\u001b[32m0\u001b[39m)\n\u001b[32m      6\u001b[39m y = train_df[\u001b[33m'\u001b[39m\u001b[33mtarget\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# Initialize a model\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/northwestern-u-msai-349-final-project-TVbZx5Bf-py3.12/lib/python3.12/site-packages/pandas/core/frame.py:5603\u001b[39m, in \u001b[36mDataFrame.drop\u001b[39m\u001b[34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[39m\n\u001b[32m   5455\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdrop\u001b[39m(\n\u001b[32m   5456\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   5457\u001b[39m     labels: IndexLabel | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   5464\u001b[39m     errors: IgnoreRaise = \u001b[33m\"\u001b[39m\u001b[33mraise\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   5465\u001b[39m ) -> DataFrame | \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   5466\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   5467\u001b[39m \u001b[33;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[32m   5468\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   5601\u001b[39m \u001b[33;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[32m   5602\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m5603\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   5604\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5605\u001b[39m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5606\u001b[39m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5607\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5608\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5609\u001b[39m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[43m=\u001b[49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5610\u001b[39m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   5611\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/northwestern-u-msai-349-final-project-TVbZx5Bf-py3.12/lib/python3.12/site-packages/pandas/core/generic.py:4810\u001b[39m, in \u001b[36mNDFrame.drop\u001b[39m\u001b[34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[39m\n\u001b[32m   4808\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes.items():\n\u001b[32m   4809\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4810\u001b[39m         obj = \u001b[43mobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4812\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[32m   4813\u001b[39m     \u001b[38;5;28mself\u001b[39m._update_inplace(obj)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/northwestern-u-msai-349-final-project-TVbZx5Bf-py3.12/lib/python3.12/site-packages/pandas/core/generic.py:4852\u001b[39m, in \u001b[36mNDFrame._drop_axis\u001b[39m\u001b[34m(self, labels, axis, level, errors, only_slice)\u001b[39m\n\u001b[32m   4850\u001b[39m         new_axis = axis.drop(labels, level=level, errors=errors)\n\u001b[32m   4851\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4852\u001b[39m         new_axis = \u001b[43maxis\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4853\u001b[39m     indexer = axis.get_indexer(new_axis)\n\u001b[32m   4855\u001b[39m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[32m   4856\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.cache/pypoetry/virtualenvs/northwestern-u-msai-349-final-project-TVbZx5Bf-py3.12/lib/python3.12/site-packages/pandas/core/indexes/base.py:7136\u001b[39m, in \u001b[36mIndex.drop\u001b[39m\u001b[34m(self, labels, errors)\u001b[39m\n\u001b[32m   7134\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m mask.any():\n\u001b[32m   7135\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m errors != \u001b[33m\"\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m-> \u001b[39m\u001b[32m7136\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlabels[mask].tolist()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m not found in axis\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   7137\u001b[39m     indexer = indexer[~mask]\n\u001b[32m   7138\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.delete(indexer)\n",
      "\u001b[31mKeyError\u001b[39m: \"['target'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# Demonstrate feature selection process\n",
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# Prepare data\n",
    "X = train_df.drop(columns=['target']).select_dtypes(include=[\"number\"]).fillna(0)\n",
    "y = train_df['target']\n",
    "\n",
    "# Initialize a model\n",
    "model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Create RFE with 3 features\n",
    "rfe = RFE(estimator=model, n_features_to_select=3, step=1)\n",
    "rfe.fit(X, y)\n",
    "\n",
    "# Get selected features\n",
    "selected_indices = rfe.get_support(indices=True)\n",
    "selected_features = X.columns[selected_indices].tolist()\n",
    "\n",
    "# Display results\n",
    "print(f\"Selected features: {selected_features}\")\n",
    "\n",
    "# Create a DataFrame with feature rankings\n",
    "feature_ranks = pd.DataFrame({\n",
    "    'Feature': X.columns,\n",
    "    'Ranking': rfe.ranking_,\n",
    "    'Selected': rfe.support_\n",
    "}).sort_values('Ranking')\n",
    "\n",
    "feature_ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
